{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8025b5fb-fd59-434f-bc3f-8494f28a5938",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b638b6-ef08-41b9-a041-45f127086797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunsin/Installs/anaconda3/envs/posigen/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from kfp import dsl\n",
    "# from kfp.v2 import compiler\n",
    "from google.cloud import aiplatform\n",
    "from typing import List, Dict\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json, os, ast, re\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd, numpy as np\n",
    "from scipy.special import softmax\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "import scrubadub, scrubadub_spacy\n",
    "\n",
    "import snowflake.connector as sc\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "\n",
    "import vertexai\n",
    "import vertexai.preview.generative_models as generative_models\n",
    "from vertexai.generative_models import GenerativeModel, GenerationConfig, Part\n",
    "\n",
    "# Sentiments\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac3429b-0929-47b5-a74c-cfc68592cd1f",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "946af2f5-518e-4ba0-ab7b-d0b1aa58862a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Temporary secrets manager\n",
    "with open(\"secrets.json\", 'r') as secrets_file:\n",
    "    secrets = json.load(secrets_file)\n",
    "aws_access_key = secrets.get(\"aws_access_key\")\n",
    "aws_secret_key = secrets.get(\"aws_secret_key\")\n",
    "\n",
    "# AWS\n",
    "s3_source_bucket = secrets.get('s3_source_bucket')\n",
    "s3_transcripts_location = secrets.get('s3_transcripts_location')\n",
    "\n",
    "# GCP\n",
    "gcp_project_id=secrets.get('gcp_project_id')\n",
    "gcp_prjct_location=secrets.get('gcp_prjct_location')\n",
    "\n",
    "# Snowflake\n",
    "private_key_file = secrets.get('snowflakegcp_rsa_key')\n",
    "private_key_file_pwd = secrets.get('snf_ssh_key_pass')\n",
    "\n",
    "conn_params = {\n",
    "    'account': secrets.get('snf_account'),\n",
    "    'user': secrets.get('snf_user'),\n",
    "    'private_key_file': secrets.get('snf_private_key_file'),\n",
    "    'private_key_file_pwd':secrets.get('snf_private_key_pwd'),\n",
    "    'warehouse': secrets.get('snf_warehouse'),\n",
    "    'database': secrets.get('snf_database'),\n",
    "    'schema': secrets.get('snf_schema')\n",
    "}\n",
    "\n",
    "# # Sentiment Scores\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "model_sentiment = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1d7215-5372-442a-a5aa-eeb293a23af0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Util Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3bc7ee-2926-48ba-935d-94c524f01794",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Misc Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f21a74-5498-4144-bb15-90c63137e182",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Initiate Master Inter and Intra Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29239bcb-462a-450b-bfb7-057149a85c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_master_dataframes():\n",
    "    if os.path.isfile(\"df_intra_calls_data.csv\"):\n",
    "        print(\"df_intra_calls_data.csv exists.\") \n",
    "        df_intra_calls_data = pd.read_csv(\"df_intra_calls_data.csv\")\n",
    "        df_intra_calls_data.CONTACT_ID = df_intra_calls_data.CONTACT_ID.astype('string')\n",
    "    else:\n",
    "        print(\"df_intra_calls_data.csv does not exists.\")\n",
    "        df_intra_calls_data = pd.DataFrame()\n",
    "\n",
    "    if os.path.isfile(\"df_inter_calls_data.csv\"):\n",
    "        print(\"df_inter_calls_data.csv exists.\")\n",
    "        df_inter_calls_data = pd.read_csv(\"df_inter_calls_data.csv\")\n",
    "        df_inter_calls_data.CONTACT_ID = df_inter_calls_data.CONTACT_ID.astype('string')\n",
    "    else:\n",
    "        print(\"df_inter_calls_data.csv does not exists.\")\n",
    "        df_inter_calls_data = pd.DataFrame()\n",
    "\n",
    "    return df_intra_calls_data, df_inter_calls_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0b3884-9917-4dca-a767-1821346c3dfc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Function: Listing Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "673b28d9-bc11-4373-936f-a62b4f05d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_new_transcripts(aws_access_key: str, aws_secret_key: str, source_bucket: str, custom_location: str, max_objects: int):\n",
    "    \"\"\"\n",
    "    Fetch audio file from S3 and return it as a BytesIO object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=aws_access_key,\n",
    "            aws_secret_access_key=aws_secret_key\n",
    "        )\n",
    "\n",
    "        print()\n",
    "        print(\"New Transcripts to process\")\n",
    "        # List files in the folder\n",
    "        response = s3_client.list_objects_v2(Bucket=s3_source_bucket, Prefix=s3_transcripts_location)\n",
    "        \n",
    "        list_transcripts = []\n",
    "        for obj in response.get('Contents', []):\n",
    "            if obj['Key'].endswith('.json'):\n",
    "                list_transcripts.append([obj['Key'], obj['LastModified']])\n",
    "                print(f\"{str(obj['LastModified']) +\": \"+ obj['Key']}\")\n",
    "                if len(list_transcripts) >= max_objects:\n",
    "                    break  # Exit the loop after printing max objects\n",
    "    \n",
    "        return list_transcripts\n",
    "\n",
    "    except ClientError as e:\n",
    "        print(f\"Error accessing S3: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f442dc8-9426-4fea-80ef-0536f84b3de6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Function: Read Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e84739f6-1bde-4609-89b1-05728ac7ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_new_transcripts(aws_access_key: str, aws_secret_key: str, s3_source_bucket: str, file_key):\n",
    "    \"\"\"\n",
    "    Read Transcript JSON content from a specific file in S3.\n",
    "    \n",
    "    :param bucket_name: Name of the S3 bucket\n",
    "    :param file_key: Full path/key of the JSON file\n",
    "    :return: Parsed JSON content\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=aws_access_key,\n",
    "            aws_secret_access_key=aws_secret_key\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        # Download the file\n",
    "        response = s3_client.get_object(Bucket=s3_source_bucket, Key=file_key)\n",
    "        \n",
    "        # Read the content\n",
    "        json_content = response['Body'].read().decode('utf-8')\n",
    "        \n",
    "        # Parse JSON\n",
    "        return json.loads(json_content)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Transcript JSON file {file_key}: {e}\")\n",
    "        print()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c36e3-63c8-4ae5-b1f0-12ca874abd90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "# Create Intra-call Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "844bcee3-af6b-4d31-a33e-908b1883e7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def millis_to_hhmmss(millis):\n",
    "    \"\"\"Convert milliseconds to mm:ss format\"\"\"\n",
    "    total_seconds = int(millis / 1000)\n",
    "    hours = total_seconds // 3600\n",
    "    minutes = total_seconds // 60\n",
    "    seconds = total_seconds % 60\n",
    "    return f\"{minutes:02d}:{seconds:02d}\"\n",
    "\n",
    "def convert_to_seconds(time_str):\n",
    "    try:\n",
    "        # Parse time string using datetime\n",
    "        time_obj = datetime.strptime(time_str, '%H:%M:%S')\n",
    "        # Convert to timedelta and extract total seconds\n",
    "        total_seconds = time_obj.minute * 60 + time_obj.second\n",
    "        return total_seconds\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def process_transcript(\n",
    "    transcript_data: dict,\n",
    "    contact_id: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Pre-process the transcript loaded from S3 Buckets:\n",
    "    1. Load the transcript as Pandas Dataframe.\n",
    "    2. Select only the necessary columns ['BeginOffsetMillis', 'EndOffsetMillis', 'ParticipantId', 'Content', 'Sentiment', 'LoudnessScore'].\n",
    "    3. Format the time in minutes and seconds.\n",
    "    4. Rename the columns for better understanding.\n",
    "    \"\"\"\n",
    "    # Load the Transcript as Pandas Dataframe\n",
    "    transcript_df = pd.json_normalize(transcript_data['Transcript'])\n",
    "\n",
    "    # Select the relevant Columns\n",
    "    columns_to_select = [\n",
    "        'BeginOffsetMillis',\n",
    "        'EndOffsetMillis',\n",
    "        'ParticipantId',\n",
    "        'Content'\n",
    "    ]\n",
    "    formatted_df = transcript_df[columns_to_select].copy()\n",
    "    \n",
    "    # Optionally rename columns to reflect their new format\n",
    "    formatted_df = formatted_df.rename(columns={\n",
    "        'BeginOffsetMillis': 'Begin_Offset',\n",
    "        'EndOffsetMillis': 'End_Offset',\n",
    "        'Content': 'caption',\n",
    "        'Sentiment': 'sentiment_label',\n",
    "        'ParticipantId': 'speaker_tag'\n",
    "    })\n",
    "\n",
    "    # Inserting the Call ID:\n",
    "    formatted_df.insert(loc=0, column='contact_id', value=contact_id)\n",
    "    formatted_df['call_language'] = transcript_data['LanguageCode']\n",
    "\n",
    "    return formatted_df\n",
    "\n",
    "def get_sentiment_label(row):\n",
    "    # Check conditions in order of priority (Positive > Negative > Neutral)\n",
    "    if row['positive'] > row['negative'] and row['positive'] > row['neutral']:\n",
    "        return 'Positive'\n",
    "    elif row['negative'] > row['positive'] and row['negative'] > row['neutral']:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "def get_sentiment_scores(text_list):\n",
    "    dict_sentiments = []\n",
    "    for text in text_list:\n",
    "        encoded_input = tokenizer(text, return_tensors='pt')\n",
    "        output = model_sentiment(**encoded_input)\n",
    "        scores = output[0][0].detach().numpy()\n",
    "        scores = np.round(np.multiply(softmax(scores), 100), 2)\n",
    "        merged_dict = dict(zip(list(config.id2label.values()), list(scores)))\n",
    "        dict_sentiments.append(merged_dict)\n",
    "\n",
    "    df_dict_sentiments = pd.DataFrame(dict_sentiments)\n",
    "    df_dict_sentiments['sentiment_lable'] = df_dict_sentiments[['positive','negative','neutral']].apply(get_sentiment_label, axis=1)\n",
    "    \n",
    "    return df_dict_sentiments\n",
    "\n",
    "def get_different_times(intra_call):\n",
    "    # Apply formatting to both time columns\n",
    "    intra_call['start_time_second'] = (intra_call['Begin_Offset'] / 1000).astype(int)\n",
    "    # intra_call['Begin_Offset'] = intra_call['Begin_Offset'].apply(millis_to_hhmmss)\n",
    "    intra_call['end_time_second'] = (intra_call['End_Offset'] / 1000).astype(int)\n",
    "    # intra_call['End_Offset'] = intra_call['End_Offset'].apply(millis_to_hhmmss)\n",
    "    intra_call['time_spoken_second'] = intra_call['end_time_second'] - intra_call['start_time_second']\n",
    "    intra_call['time_spoken_second'] = intra_call['time_spoken_second'].where(intra_call['time_spoken_second'] >= 0, 0)\n",
    "    intra_call['time_spoken_second'] = intra_call['time_spoken_second'].fillna(0).astype(int)\n",
    "    intra_call['time_silence_second'] = intra_call['start_time_second'].shift(-1) - intra_call['end_time_second']\n",
    "    intra_call['time_silence_second'] = intra_call['time_silence_second'].where(intra_call['time_silence_second'] >= 0, 0)\n",
    "    intra_call['time_silence_second'] = intra_call['time_silence_second'].fillna(0).astype(int)\n",
    "    intra_call['load_date'] = datetime.now()\n",
    "\n",
    "    # Dropping time formatted columns\n",
    "    intra_call = intra_call.drop(['Begin_Offset', 'End_Offset'], axis=1)\n",
    "\n",
    "    return intra_call\n",
    "    \n",
    "def create_intra_call_df(aws_access_key: str, aws_secret_key: str, transcript_data: dict, contact_id: str):\n",
    "    print(f\"Creating Intra Call df\")\n",
    "        \n",
    "    # Get the relevant columns from the loaded transcript file\n",
    "    intra_call = process_transcript(transcript_data, contact_id)\n",
    "    \n",
    "    df_sentiment_scores = get_sentiment_scores(intra_call.caption.to_list())\n",
    "    intra_call = pd.concat([intra_call, df_sentiment_scores], axis=1)\n",
    "\n",
    "    intra_call = get_different_times(intra_call)\n",
    "    print(f\"Completed Intra Call df\")\n",
    "    print()\n",
    "\n",
    "    return intra_call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a3e107-9441-4b0c-a9aa-52afc93f0750",
   "metadata": {},
   "source": [
    "# Create Inter-call Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75d8a881-3e83-4780-94e6-94f7c907a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic models for validation\n",
    "class CallSummary(BaseModel):\n",
    "    summary: str = Field(..., max_length=500)\n",
    "    key_points: List[str] = Field(..., max_items=5)\n",
    "    outcome: str = Field(..., max_length=200)\n",
    "    follow_up_recommendations: List[str] = Field(..., max_items=3)\n",
    "\n",
    "class CallTopic(BaseModel):\n",
    "    primary_topic: str = Field(..., max_length=100)\n",
    "    category: str = Field(..., max_length=100)\n",
    "    sub_category: str = Field(..., max_length=100)\n",
    "\n",
    "class AgentCoaching(BaseModel):\n",
    "    strengths: List[str] = Field(..., max_items=3)\n",
    "    improvement_areas: List[str] = Field(..., max_items=3)\n",
    "    specific_recommendations: List[str] = Field(..., max_items=4)\n",
    "    skill_development_focus: List[str] = Field(..., max_items=3)\n",
    "\n",
    "class TranscriptAnalysis(BaseModel):\n",
    "    call_summary: CallSummary\n",
    "    call_topic: CallTopic\n",
    "    agent_coaching: AgentCoaching\n",
    "\n",
    "class KPIExtractor:\n",
    "    def __init__(self, project_id: str, location: str):\n",
    "        vertexai.init(project=project_id, location=location)\n",
    "        self.model = GenerativeModel(\"gemini-1.5-flash-002\")\n",
    "        self.generation_config = {\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_output_tokens\": 1024,\n",
    "            \"top_p\": 0.8,\n",
    "            \"top_k\": 40,\n",
    "            \"response_format\": \"json\"\n",
    "        }\n",
    "        self.safety_settings = {\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "        }\n",
    "        \n",
    "    def create_prompt(self, transcript: str) -> str:\n",
    "        \"\"\"Create a structured prompt for KPI extraction\"\"\"\n",
    "        return f\"\"\"\n",
    "        Analyze this call transcript and provide a structured analysis in the exact JSON format specified below.\n",
    "        Keep responses concise, specific, and actionable.\n",
    "\n",
    "        Guidelines:\n",
    "        - Call summary should be factual and highlight key interactions\n",
    "        - Topics and categories should match standard business taxonomies\n",
    "        - Coaching points should be specific and actionable\n",
    "        - All responses must follow the exact structure specified\n",
    "        - Ensure all lists have the specified maximum number of items\n",
    "        - All text fields must be clear, professional, and free of fluff\n",
    "\n",
    "        Transcript:\n",
    "        {transcript}\n",
    "\n",
    "        Required Output Structure:\n",
    "        {{\n",
    "            \"call_summary\": {{\n",
    "                \"summary\": \"3-4 line overview of the call\",\n",
    "                \"key_points\": [\"Point 1\", \"Point 2\", \"Point 3\", \"Point 4\", \"Point 5\"],\n",
    "                \"outcome\": \"Clear statement of call resolution\",\n",
    "                \"follow_up_recommendations\": [\"Rec 1\", \"Rec 2\", \"Rec 3\"]\n",
    "            }},\n",
    "            \"call_topic\": {{\n",
    "                \"primary_topic\": \"Main topic of discussion\",\n",
    "                \"category\": \"Business category\",\n",
    "                \"sub_category\": \"Specific sub-category\"\n",
    "            }},\n",
    "            \"agent_coaching\": {{\n",
    "                \"strengths\": [\"Strength 1\", \"Strength 2\", \"Strength 3\"],\n",
    "                \"improvement_areas\": [\"Area 1\", \"Area 2\", \"Area 3\"],\n",
    "                \"specific_recommendations\": [\"Rec 1\", \"Rec 2\", \"Rec 3\", \"Rec 4\"],\n",
    "                \"skill_development_focus\": [\"Skill 1\", \"Skill 2\", \"Skill 3\"]\n",
    "            }}\n",
    "        }}\n",
    "\n",
    "        Rules:\n",
    "        1. Maintain exact JSON structure\n",
    "        2. No additional fields or comments\n",
    "        3. No markdown formatting\n",
    "        4. Ensure all arrays have the exact number of items specified\n",
    "        5. Keep all text concise and professional\n",
    "        6. Do not mention any PII information such as Customer Name etc.\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    def extract_json(self, response: str):\n",
    "        \"\"\"Extracts valid JSON from a response that may contain extra characters like ```json.\"\"\"\n",
    "        match = re.search(r'```json\\s*([\\s\\S]*?)\\s*```', response)\n",
    "        if match:\n",
    "            json_str = match.group(1)  # Extract JSON content\n",
    "        else:\n",
    "            json_str = response.strip()  # If no markdown, assume raw JSON\n",
    "        \n",
    "        try:\n",
    "            return json.loads(json_str)  # Convert to dictionary\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(\"Invalid JSON response\")\n",
    "    \n",
    "            \n",
    "    def validate_response(self, response_json: Dict) -> TranscriptAnalysis:\n",
    "        \"\"\"Validate the response using Pydantic models\"\"\"\n",
    "        try:\n",
    "            return TranscriptAnalysis(**response_json)\n",
    "        except ValidationError as e:\n",
    "            print(f\"Skipping call {i + 1}: Error extracting KPIs - {e}\")\n",
    "\n",
    "    def extract_genai_kpis(self, transcript: str):\n",
    "        \"\"\"\n",
    "        Extract KPIs from transcript using Gemini API\n",
    "        \n",
    "        Args:\n",
    "            transcript (str): Call transcript text\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Structured KPI data or None if extraction fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Generate prompt\n",
    "            prompt = self.create_prompt(transcript)\n",
    "            \n",
    "            # Get response from Gemini\n",
    "            response = self.model.generate_content(prompt)\n",
    "            \n",
    "            # Parse JSON response\n",
    "            response_json = self.extract_json(response.text)\n",
    "            \n",
    "            # Validate response structure\n",
    "            validated_response = self.validate_response(response_json)\n",
    "            \n",
    "            return validated_response.model_dump()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting KPIs: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "def dict_to_newline_string(data: dict) -> str:\n",
    "    \"\"\"Converts a dictionary into a new-line formatted string.\"\"\"\n",
    "    formatted_str = \"\"\n",
    "    for key, value in data.items():\n",
    "        formatted_str += f\"{key}:\\n\"\n",
    "        for item in value:\n",
    "            formatted_str += f\"  - {item}\\n\"\n",
    "    return formatted_str.strip()\n",
    "    \n",
    "def create_inter_call_df(\n",
    "    gcp_project_id: str,\n",
    "    gcp_prjct_location: str,\n",
    "    df_intra_call: pd.DataFrame,\n",
    "    transcript_data: dict,\n",
    "    ac_last_modified_date: datetime\n",
    "):\n",
    "    # Redact PII Data\n",
    "    print(\"Redacting PII data.\")\n",
    "    scrubber = scrubadub.Scrubber()\n",
    "    scrubber.add_detector(scrubadub_spacy.detectors.SpacyEntityDetector)\n",
    "    df_intra_call.caption = df_intra_call.caption.apply(scrubber.clean)\n",
    "\n",
    "    # Extract KPIs from Gemini\n",
    "    extractor = KPIExtractor(gcp_project_id, gcp_prjct_location)\n",
    "    transcript = \" \".join(df_intra_call.caption)\n",
    "    call_gen_kpis = extractor.extract_genai_kpis(transcript)\n",
    "\n",
    "    print(f\"Creating Inter Call df\")\n",
    "    inter_call_dict = {}\n",
    "    inter_call_dict['contact_id'] = str(df_intra_call['contact_id'][0])\n",
    "    inter_call_dict['call_text'] = \" \".join(df_intra_call.caption)\n",
    "    inter_call_dict['call_summary'] = call_gen_kpis['call_summary']['summary']\n",
    "    inter_call_dict['topic'] = call_gen_kpis['call_topic']['primary_topic']\n",
    "    inter_call_dict['category'] = \"Static Category TBD\"\n",
    "    inter_call_dict['category_generated'] = call_gen_kpis['call_topic']['category']\n",
    "    inter_call_dict['sub_category'] = \"Static Sub-Category TBD\"\n",
    "    inter_call_dict['sub_category_generated'] = call_gen_kpis['call_topic']['sub_category']\n",
    "    inter_call_dict['agent_coaching'] = dict_to_newline_string(call_gen_kpis['agent_coaching'])\n",
    "\n",
    "    df_inter_call = pd.DataFrame(pd.Series(inter_call_dict)).T\n",
    "\n",
    "    # Add metadata from AWS\n",
    "    # df_inter_call['account_id'] = transcript_data['AccountId']\n",
    "    df_inter_call['agent_speech_speed'] = transcript_data['ConversationCharacteristics']['TalkSpeed']['DetailsByParticipant']['AGENT']['AverageWordsPerMinute']\n",
    "    df_inter_call['customer_speech_speed'] = transcript_data['ConversationCharacteristics']['TalkSpeed']['DetailsByParticipant']['CUSTOMER']['AverageWordsPerMinute']\n",
    "    df_inter_call['total_talktime_agent_second'] = transcript_data['ConversationCharacteristics']['TalkTime']['DetailsByParticipant']['AGENT']['TotalTimeMillis']\n",
    "    df_inter_call['total_talktime_customer_second'] = transcript_data['ConversationCharacteristics']['TalkTime']['DetailsByParticipant']['CUSTOMER']['TotalTimeMillis']\n",
    "    df_inter_call['total_talktime_call_second'] = transcript_data['ConversationCharacteristics']['TalkTime']['TotalTimeMillis']\n",
    "    df_inter_call['total_duration_call_second'] = transcript_data['ConversationCharacteristics']['TotalConversationDurationMillis']\n",
    "    df_inter_call['total_dead_air_call_second'] = df_inter_call['total_duration_call_second'] - df_inter_call['total_talktime_call_second']\n",
    "    # df_inter_call['customer_instance_id'] = transcript_data['CustomerMetadata']['InstanceId']\n",
    "    # df_inter_call['call_job_status'] = transcript_data['JobStatus']\n",
    "    df_inter_call['call_language'] = transcript_data['LanguageCode']\n",
    "    df_inter_call['call_s3_uri'] = transcript_data['CustomerMetadata']['InputS3Uri']\n",
    "    df_inter_call['ac_last_modified_date'] = ac_last_modified_date\n",
    "    df_inter_call['load_date'] = datetime.now()\n",
    "    \n",
    "    print(f\"Completed Inter Call df\")\n",
    "    print()\n",
    "    \n",
    "    return df_inter_call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cc4160-5de2-42de-bd6f-e66070409da1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Writing Dataframe to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fb02876-9062-47ca-890b-feace9e4e210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_new_records(conn, table_name, df):\n",
    "    \"\"\"\n",
    "    Inserts only new records (based on ID) into Snowflake table.\n",
    "    1. Fetches existing IDs from table\n",
    "    2. Filters out rows with existing IDs from DataFrame\n",
    "    3. Inserts only new records\n",
    "    \n",
    "    Args:\n",
    "        conn: Snowflake connection object\n",
    "        table_name: Name of the target table\n",
    "        df: Pandas DataFrame containing the data (must have 'id' column)\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Get existing IDs from Snowflake table\n",
    "    cursor.execute(f\"SELECT DISTINCT(CONTACT_ID) FROM {table_name}\")\n",
    "    existing_ids = {row[0] for row in cursor.fetchall()}\n",
    "    \n",
    "    # Filter DataFrame to keep only new records\n",
    "    new_records_df = df[~df['CONTACT_ID'].isin(existing_ids)]\n",
    "    \n",
    "    if len(new_records_df) == 0:\n",
    "        print(\"No new records to insert\")\n",
    "        return 0\n",
    "    \n",
    "    # Insert new records\n",
    "    success, nchunks, nrows, _ = write_pandas(conn, new_records_df, table_name)\n",
    "    \n",
    "    print(f\"Inserted {nrows} new records\")\n",
    "    print(f\"Skipped {len(df) - len(new_records_df)} existing records\")\n",
    "    \n",
    "    cursor.close()\n",
    "    return nrows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d121fa-be88-424a-9d70-507b203a1196",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb7b03f4-87ed-4f08-8328-5006e4534573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_intra_calls_data.csv exists.\n",
      "df_inter_calls_data.csv exists.\n",
      "\n",
      "New Transcripts to process\n",
      "2025-01-30 09:05:04+00:00: connect-audio-files/to_process_transcripts/309a0db8-2735-4537-988a-a66bff37c159_analysis_2025-01-17T21_06_34Z.json\n",
      "2025-01-30 09:05:03+00:00: connect-audio-files/to_process_transcripts/3c2ddea2-d6e3-4acd-8abd-6eb98c192bd8_analysis_2025-01-06T18_29_29Z.json\n",
      "2025-01-30 09:05:02+00:00: connect-audio-files/to_process_transcripts/3e747be8-b57e-4e90-954d-dabc1e9a623a_analysis_2025-01-14T19_30_16Z.json\n",
      "2025-01-30 09:04:52+00:00: connect-audio-files/to_process_transcripts/4a0a35d3-6221-4ccd-be93-83938c24a544_analysis_2025-01-17T19_04_29Z.json\n",
      "2025-01-30 09:05:05+00:00: connect-audio-files/to_process_transcripts/4abe04de-5d7d-4eb9-a999-99d1c6ef54c1_analysis_2024-12-19T17_43_08Z.json\n",
      "2025-01-30 09:04:59+00:00: connect-audio-files/to_process_transcripts/4d8deee2-8062-40e8-b580-d45d79e94abe_analysis_2025-01-15T22_57_07Z.json\n",
      "2025-01-30 09:04:56+00:00: connect-audio-files/to_process_transcripts/515b317f-21d0-4aae-b88b-b90f00240a13_analysis_2025-01-18T21_53_50Z.json\n",
      "2025-01-30 09:05:00+00:00: connect-audio-files/to_process_transcripts/868375c1-2111-4990-b9d2-36693c7bad46_analysis_2025-01-06T18_29_18Z.json\n",
      "2025-01-30 09:04:57+00:00: connect-audio-files/to_process_transcripts/a0043306-ab8c-4e7f-9839-380a73285d8f_analysis_2025-01-22T15_09_32Z.json\n",
      "2025-01-30 09:05:05+00:00: connect-audio-files/to_process_transcripts/b65e1cdc-bedb-40b8-8a69-800cdc8ec8b9_analysis_2025-01-06T17_37_39Z.json\n",
      "2025-01-30 09:04:54+00:00: connect-audio-files/to_process_transcripts/bedd3484-4bbd-4628-8465-42c80d78903d_analysis_2024-11-14T16_16_37Z.json\n",
      "2025-01-30 09:05:04+00:00: connect-audio-files/to_process_transcripts/c1e5d795-eae6-4489-afb9-78367b7893fb_analysis_2025-01-06T17_42_57Z.json\n",
      "2025-01-30 09:05:01+00:00: connect-audio-files/to_process_transcripts/c4d6ee3a-5bd4-4b74-9d34-169b4e622375_analysis_2025-01-18T17_58_23Z.json\n",
      "2025-01-30 09:04:57+00:00: connect-audio-files/to_process_transcripts/ccc17bad-5edb-4a9c-b061-9ac7d37300cc_analysis_2025-01-06T18_08_50Z.json\n",
      "2025-01-30 09:04:53+00:00: connect-audio-files/to_process_transcripts/e470ef57-3e44-4cd5-8a5b-dfcb54566ab0_analysis_2025-01-22T13_42_06Z.json\n",
      "2025-01-30 09:05:03+00:00: connect-audio-files/to_process_transcripts/eb54bd05-adba-4ee3-9bc0-7bc17d01aba9_analysis_2025-01-06T18_29_57Z.json\n",
      "2025-01-30 09:04:58+00:00: connect-audio-files/to_process_transcripts/f59ccd68-00d6-450d-8599-57666c9975a6_analysis_2024-12-31T22_23_31Z.json\n",
      "2025-01-30 09:05:01+00:00: connect-audio-files/to_process_transcripts/fbe2b590-261d-4e07-8417-a4294693001e_analysis_2025-01-16T20_38_40Z.json\n",
      "\n",
      "Transcripts to process: 18\n",
      "\n",
      "\n",
      "\n",
      "Processing Call: 309a0db8-2735-4537-988a-a66bff37c159\n",
      "Call already Processed.\n",
      "\n",
      "\n",
      "\n",
      "Processing Call: 3c2ddea2-d6e3-4acd-8abd-6eb98c192bd8\n",
      "Call already Processed.\n",
      "\n",
      "\n",
      "\n",
      "Processing Call: 3e747be8-b57e-4e90-954d-dabc1e9a623a\n",
      "Call already Processed.\n",
      "\n",
      "\n",
      "\n",
      "Processing Call: 4a0a35d3-6221-4ccd-be93-83938c24a544\n",
      "Call already Processed.\n",
      "\n",
      "\n",
      "\n",
      "Processing Call: 4abe04de-5d7d-4eb9-a999-99d1c6ef54c1\n",
      "Call already Processed.\n",
      "\n",
      "\n",
      "\n",
      "Processing Call: 4d8deee2-8062-40e8-b580-d45d79e94abe\n",
      "Call already Processed.\n",
      "\n",
      "\n",
      "\n",
      "Processing Call: 515b317f-21d0-4aae-b88b-b90f00240a13\n",
      "Call already Processed.\n",
      "\n",
      "\n",
      "\n",
      "Processing Call: 868375c1-2111-4990-b9d2-36693c7bad46\n",
      "Call already Processed.\n",
      "\n",
      "\n",
      "\n",
      "Processing Call: a0043306-ab8c-4e7f-9839-380a73285d8f\n",
      "Call already Processed.\n",
      "\n",
      "\n",
      "\n",
      "Processing Call: b65e1cdc-bedb-40b8-8a69-800cdc8ec8b9\n",
      "Call already Processed.\n",
      "\n",
      "\n",
      "\n",
      "Processing Call: bedd3484-4bbd-4628-8465-42c80d78903d\n",
      "Call already Processed.\n",
      "\n",
      "\n",
      "\n",
      "Processing Call: c1e5d795-eae6-4489-afb9-78367b7893fb\n",
      "Call already Processed.\n",
      "\n",
      "\n",
      "\n",
      "Processing Call: c4d6ee3a-5bd4-4b74-9d34-169b4e622375\n",
      "Call already Processed.\n",
      "\n",
      "\n",
      "\n",
      "Processing Call: ccc17bad-5edb-4a9c-b061-9ac7d37300cc\n",
      "Call already Processed.\n",
      "\n",
      "\n",
      "\n",
      "Processing Call: e470ef57-3e44-4cd5-8a5b-dfcb54566ab0\n",
      "Call already Processed.\n",
      "\n",
      "\n",
      "\n",
      "Processing Call: eb54bd05-adba-4ee3-9bc0-7bc17d01aba9\n",
      "Call already Processed.\n",
      "\n",
      "\n",
      "\n",
      "Processing Call: f59ccd68-00d6-450d-8599-57666c9975a6\n",
      "Call already Processed.\n",
      "\n",
      "\n",
      "\n",
      "Processing Call: fbe2b590-261d-4e07-8417-a4294693001e\n",
      "Call already Processed.\n",
      "\n",
      "Writing Dataframe to Snowflake.\n",
      "Inserted 18 new records\n",
      "Skipped 0 existing records\n",
      "Completed processing 18 Calls\n"
     ]
    }
   ],
   "source": [
    "# Initiating Master DataFrames\n",
    "df_intra_calls_data, df_inter_calls_data = initiate_master_dataframes()\n",
    "\n",
    "# Get the transcripts in to_process_folder\n",
    "max_objects = 20\n",
    "list_transcripts = list_new_transcripts(aws_access_key, aws_secret_key, s3_source_bucket, s3_transcripts_location, max_objects)\n",
    "\n",
    "# If there are transcripts to be processed\n",
    "if len(list_transcripts) == 0:\n",
    "    print(\"No Transcripts to Process\")\n",
    "    print()\n",
    "\n",
    "else:\n",
    "    print()\n",
    "    print(f\"Transcripts to process: {len(list_transcripts)}\")\n",
    "    print()\n",
    "    # Process the call\n",
    "    for transcript in list_transcripts:\n",
    "        # get the call ID\n",
    "        contact_id = transcript[0].split('/')[-1].split('.')[0].split('analysis')[0].strip('_')\n",
    "        ac_last_modified_date = datetime.strptime(transcript[0].split('analysis_')[-1].split('.')[0].replace('_', ':'), '%Y-%m-%dT%H:%M:%SZ')\n",
    "        print()\n",
    "        print()\n",
    "        print(f\"Processing Call: {contact_id}\")        \n",
    "\n",
    "        # Check if Call Already Processed\n",
    "        if (len(df_intra_calls_data) > 0 and contact_id in df_intra_calls_data.CONTACT_ID.unique()) and (len(df_inter_calls_data) > 0 and contact_id in df_inter_calls_data.CONTACT_ID.unique()):\n",
    "            print(\"Call already Processed.\")\n",
    "            print()\n",
    "            # break\n",
    "\n",
    "        else:\n",
    "            # get the audio transcript file name\n",
    "            transcript_file = transcript[0]\n",
    "            \n",
    "            # Get the Transcript file from S3 Bucket\n",
    "            transcript_data = read_new_transcripts(aws_access_key, aws_secret_key, s3_source_bucket, transcript_file)\n",
    "            print(\"Successfully loaded the Transcript JSON\")\n",
    "            print()\n",
    "        \n",
    "            # Create the Inter Call KPIs\n",
    "            df_intra_call = create_intra_call_df(aws_access_key, aws_secret_key, transcript_data, contact_id)\n",
    "        \n",
    "            # Create the Intra Call KPIs\n",
    "            df_inter_call = create_inter_call_df(gcp_project_id, gcp_prjct_location, df_intra_call, transcript_data, ac_last_modified_date)\n",
    "\n",
    "            ###============================================================###\n",
    "            # Appending to Intra-calls Master DataFrame\n",
    "            df_intra_call.columns = df_intra_call.columns.str.upper()  # Capitalising Column names for Snowflake\n",
    "            df_intra_calls_data = pd.concat([df_intra_calls_data, df_intra_call], ignore_index=True)\n",
    "            df_intra_calls_data.to_csv(\"df_intra_calls_data.csv\", index=False)\n",
    "            print(\"Completed Persisting df_intra_calls_data with \"+str(contact_id)+\" to CSV.\")\n",
    "            \n",
    "            # Appending to Inter-calls Master DataFrame\n",
    "            df_inter_call.columns = df_inter_call.columns.str.upper()  # Capitalising Column names for Snowflake\n",
    "            df_inter_calls_data = pd.concat([df_inter_calls_data, df_inter_call], ignore_index=True)\n",
    "            df_inter_calls_data.to_csv(\"df_inter_calls_data.csv\", index=False)\n",
    "            print(\"Completed Persisting df_inter_calls_data Dataframe to CSV.\")\n",
    "\n",
    "    print(f\"Writing Dataframe to Snowflake.\")\n",
    "    conn = sc.connect(**conn_params)\n",
    "    table_name ='SRC_GCP_INTER_CALLS'\n",
    "    insert_new_records(conn, table_name, df_inter_calls_data)\n",
    "    \n",
    "    table_name ='SRC_GCP_INTRA_CALLS'\n",
    "    success, nchunks, nrows, _ = write_pandas(conn, df_intra_calls_data, 'SRC_GCP_INTRA_CALLS', auto_create_table=True)\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"Completed processing {len(list_transcripts)} Calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcec95e-0de5-456c-a3d5-c8a45b6f3046",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c4966b6-3033-476f-902b-8a95802ead7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inter_calls_data = df_inter_calls_data.drop(['CALL_JOB_STATUS'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "661c6a10-daf0-4f3a-8af0-dcec813a6409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONTACT_ID</th>\n",
       "      <th>CALL_TEXT</th>\n",
       "      <th>CALL_SUMMARY</th>\n",
       "      <th>TOPIC</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>CATEGORY_GENERATED</th>\n",
       "      <th>SUB_CATEGORY</th>\n",
       "      <th>SUB_CATEGORY_GENERATED</th>\n",
       "      <th>AGENT_COACHING</th>\n",
       "      <th>AGENT_SPEECH_SPEED</th>\n",
       "      <th>CUSTOMER_SPEECH_SPEED</th>\n",
       "      <th>TOTAL_TALKTIME_AGENT_SECOND</th>\n",
       "      <th>TOTAL_TALKTIME_CUSTOMER_SECOND</th>\n",
       "      <th>TOTAL_TALKTIME_CALL_SECOND</th>\n",
       "      <th>TOTAL_DURATION_CALL_SECOND</th>\n",
       "      <th>TOTAL_DEAD_AIR_CALL_SECOND</th>\n",
       "      <th>CALL_LANGUAGE</th>\n",
       "      <th>CALL_S3_URI</th>\n",
       "      <th>AC_LAST_MODIFIED_DATE</th>\n",
       "      <th>LOAD_DATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>309a0db8-2735-4537-988a-a66bff37c159</td>\n",
       "      <td>Hi, I'm looking for Mr. {{NAME}}. Speaking. Hi...</td>\n",
       "      <td>The call involved transferring a solar lease t...</td>\n",
       "      <td>Solar Lease Transfer</td>\n",
       "      <td>Static Category TBD</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>Static Sub-Category TBD</td>\n",
       "      <td>Account Management</td>\n",
       "      <td>strengths:\\n  - Clearly explained complex info...</td>\n",
       "      <td>221</td>\n",
       "      <td>155</td>\n",
       "      <td>627258</td>\n",
       "      <td>321628</td>\n",
       "      <td>948886</td>\n",
       "      <td>1044130</td>\n",
       "      <td>95244</td>\n",
       "      <td>en-US</td>\n",
       "      <td>s3://amazon-connect-39f6aa5d9242/connect/posig...</td>\n",
       "      <td>2025-01-17 21:06:34</td>\n",
       "      <td>2025-02-11 17:33:41.220487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3c2ddea2-d6e3-4acd-8abd-6eb98c192bd8</td>\n",
       "      <td>Thank you for calling {{ORGANIZATION}}. This i...</td>\n",
       "      <td>Customer called to make a payment of $64.99.  ...</td>\n",
       "      <td>Payment Processing</td>\n",
       "      <td>Static Category TBD</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Static Sub-Category TBD</td>\n",
       "      <td>Payment Collection</td>\n",
       "      <td>strengths:\\n  - Successfully processed payment...</td>\n",
       "      <td>160</td>\n",
       "      <td>121</td>\n",
       "      <td>164142</td>\n",
       "      <td>240297</td>\n",
       "      <td>404439</td>\n",
       "      <td>475679</td>\n",
       "      <td>71240</td>\n",
       "      <td>en-US</td>\n",
       "      <td>s3://amazon-connect-39f6aa5d9242/connect/posig...</td>\n",
       "      <td>2025-01-06 18:29:29</td>\n",
       "      <td>2025-02-11 17:34:01.648864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3e747be8-b57e-4e90-954d-dabc1e9a623a</td>\n",
       "      <td>Hello, Ms. {{NAME}}, uh, for some reason we wa...</td>\n",
       "      <td>Customer called regarding inaccurate informati...</td>\n",
       "      <td>Sales Misinformation and Complaint</td>\n",
       "      <td>Static Category TBD</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>Static Sub-Category TBD</td>\n",
       "      <td>Sales Issue</td>\n",
       "      <td>strengths:\\n  - Empathetic listening and under...</td>\n",
       "      <td>167</td>\n",
       "      <td>192</td>\n",
       "      <td>407773</td>\n",
       "      <td>242444</td>\n",
       "      <td>650217</td>\n",
       "      <td>665429</td>\n",
       "      <td>15212</td>\n",
       "      <td>en-US</td>\n",
       "      <td>s3://amazon-connect-39f6aa5d9242/connect/posig...</td>\n",
       "      <td>2025-01-14 19:30:16</td>\n",
       "      <td>2025-02-11 17:35:11.514886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4a0a35d3-6221-4ccd-be93-83938c24a544</td>\n",
       "      <td>Hello? Good afternoon. My name is {{NAME}}. I'...</td>\n",
       "      <td>A customer called regarding a lease transfer. ...</td>\n",
       "      <td>Lease Transfer</td>\n",
       "      <td>Static Category TBD</td>\n",
       "      <td>Real Estate</td>\n",
       "      <td>Static Sub-Category TBD</td>\n",
       "      <td>Tenant Services</td>\n",
       "      <td>strengths:\\n  - Successfully clarified custome...</td>\n",
       "      <td>184</td>\n",
       "      <td>179</td>\n",
       "      <td>68808</td>\n",
       "      <td>38579</td>\n",
       "      <td>107387</td>\n",
       "      <td>118739</td>\n",
       "      <td>11352</td>\n",
       "      <td>en-US</td>\n",
       "      <td>s3://amazon-connect-39f6aa5d9242/connect/posig...</td>\n",
       "      <td>2025-01-17 19:04:29</td>\n",
       "      <td>2025-02-11 17:35:23.402853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4abe04de-5d7d-4eb9-a999-99d1c6ef54c1</td>\n",
       "      <td>Good, good morning. This is {{NAME}} with {{OR...</td>\n",
       "      <td>Customer reported high electricity bills despi...</td>\n",
       "      <td>Billing Discrepancy/Solar System Malfunction</td>\n",
       "      <td>Static Category TBD</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>Static Sub-Category TBD</td>\n",
       "      <td>Technical Issue/Billing Inquiry</td>\n",
       "      <td>strengths:\\n  - Patiently explained complex bi...</td>\n",
       "      <td>144</td>\n",
       "      <td>178</td>\n",
       "      <td>486982</td>\n",
       "      <td>340959</td>\n",
       "      <td>827941</td>\n",
       "      <td>1423500</td>\n",
       "      <td>595559</td>\n",
       "      <td>en-US</td>\n",
       "      <td>s3://amazon-connect-39f6aa5d9242/connect/posig...</td>\n",
       "      <td>2024-12-19 17:43:08</td>\n",
       "      <td>2025-02-11 17:35:54.439372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             CONTACT_ID  \\\n",
       "0  309a0db8-2735-4537-988a-a66bff37c159   \n",
       "1  3c2ddea2-d6e3-4acd-8abd-6eb98c192bd8   \n",
       "2  3e747be8-b57e-4e90-954d-dabc1e9a623a   \n",
       "3  4a0a35d3-6221-4ccd-be93-83938c24a544   \n",
       "4  4abe04de-5d7d-4eb9-a999-99d1c6ef54c1   \n",
       "\n",
       "                                           CALL_TEXT  \\\n",
       "0  Hi, I'm looking for Mr. {{NAME}}. Speaking. Hi...   \n",
       "1  Thank you for calling {{ORGANIZATION}}. This i...   \n",
       "2  Hello, Ms. {{NAME}}, uh, for some reason we wa...   \n",
       "3  Hello? Good afternoon. My name is {{NAME}}. I'...   \n",
       "4  Good, good morning. This is {{NAME}} with {{OR...   \n",
       "\n",
       "                                        CALL_SUMMARY  \\\n",
       "0  The call involved transferring a solar lease t...   \n",
       "1  Customer called to make a payment of $64.99.  ...   \n",
       "2  Customer called regarding inaccurate informati...   \n",
       "3  A customer called regarding a lease transfer. ...   \n",
       "4  Customer reported high electricity bills despi...   \n",
       "\n",
       "                                          TOPIC             CATEGORY  \\\n",
       "0                          Solar Lease Transfer  Static Category TBD   \n",
       "1                            Payment Processing  Static Category TBD   \n",
       "2            Sales Misinformation and Complaint  Static Category TBD   \n",
       "3                                Lease Transfer  Static Category TBD   \n",
       "4  Billing Discrepancy/Solar System Malfunction  Static Category TBD   \n",
       "\n",
       "  CATEGORY_GENERATED             SUB_CATEGORY  \\\n",
       "0   Customer Service  Static Sub-Category TBD   \n",
       "1            Finance  Static Sub-Category TBD   \n",
       "2   Customer Service  Static Sub-Category TBD   \n",
       "3        Real Estate  Static Sub-Category TBD   \n",
       "4   Customer Service  Static Sub-Category TBD   \n",
       "\n",
       "            SUB_CATEGORY_GENERATED  \\\n",
       "0               Account Management   \n",
       "1               Payment Collection   \n",
       "2                      Sales Issue   \n",
       "3                  Tenant Services   \n",
       "4  Technical Issue/Billing Inquiry   \n",
       "\n",
       "                                      AGENT_COACHING  AGENT_SPEECH_SPEED  \\\n",
       "0  strengths:\\n  - Clearly explained complex info...                 221   \n",
       "1  strengths:\\n  - Successfully processed payment...                 160   \n",
       "2  strengths:\\n  - Empathetic listening and under...                 167   \n",
       "3  strengths:\\n  - Successfully clarified custome...                 184   \n",
       "4  strengths:\\n  - Patiently explained complex bi...                 144   \n",
       "\n",
       "   CUSTOMER_SPEECH_SPEED  TOTAL_TALKTIME_AGENT_SECOND  \\\n",
       "0                    155                       627258   \n",
       "1                    121                       164142   \n",
       "2                    192                       407773   \n",
       "3                    179                        68808   \n",
       "4                    178                       486982   \n",
       "\n",
       "   TOTAL_TALKTIME_CUSTOMER_SECOND  TOTAL_TALKTIME_CALL_SECOND  \\\n",
       "0                          321628                      948886   \n",
       "1                          240297                      404439   \n",
       "2                          242444                      650217   \n",
       "3                           38579                      107387   \n",
       "4                          340959                      827941   \n",
       "\n",
       "   TOTAL_DURATION_CALL_SECOND  TOTAL_DEAD_AIR_CALL_SECOND CALL_LANGUAGE  \\\n",
       "0                     1044130                       95244         en-US   \n",
       "1                      475679                       71240         en-US   \n",
       "2                      665429                       15212         en-US   \n",
       "3                      118739                       11352         en-US   \n",
       "4                     1423500                      595559         en-US   \n",
       "\n",
       "                                         CALL_S3_URI AC_LAST_MODIFIED_DATE  \\\n",
       "0  s3://amazon-connect-39f6aa5d9242/connect/posig...   2025-01-17 21:06:34   \n",
       "1  s3://amazon-connect-39f6aa5d9242/connect/posig...   2025-01-06 18:29:29   \n",
       "2  s3://amazon-connect-39f6aa5d9242/connect/posig...   2025-01-14 19:30:16   \n",
       "3  s3://amazon-connect-39f6aa5d9242/connect/posig...   2025-01-17 19:04:29   \n",
       "4  s3://amazon-connect-39f6aa5d9242/connect/posig...   2024-12-19 17:43:08   \n",
       "\n",
       "                    LOAD_DATE  \n",
       "0  2025-02-11 17:33:41.220487  \n",
       "1  2025-02-11 17:34:01.648864  \n",
       "2  2025-02-11 17:35:11.514886  \n",
       "3  2025-02-11 17:35:23.402853  \n",
       "4  2025-02-11 17:35:54.439372  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inter_calls_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05029c94-1906-402b-96ee-5898258c39e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Dataframe to Snowflake.\n",
      "Inserted 18 new records\n",
      "Skipped 0 existing records\n"
     ]
    }
   ],
   "source": [
    "print(f\"Writing Dataframe to Snowflake.\")\n",
    "conn = sc.connect(**conn_params)\n",
    "table_name ='SRC_GCP_INTER_CALLS'\n",
    "insert_new_records(conn, table_name, df_inter_calls_data)\n",
    "\n",
    "table_name ='SRC_GCP_INTRA_CALLS'\n",
    "success, nchunks, nrows, _ = write_pandas(conn, df_intra_calls_data, 'SRC_GCP_INTRA_CALLS', auto_create_table=True)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5cf305-e9f0-4439-a4a9-8b5ea21fdbd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "posigen",
   "language": "python",
   "name": "posiegen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
