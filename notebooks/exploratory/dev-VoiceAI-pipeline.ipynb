{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice AI Pipeline using Vertex AI\n",
    "\n",
    "This notebook transforms the original voice analysis script into a Vertex AI Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from google.cloud import aiplatform\n",
    "from typing import List, Dict\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json, os, ast, re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import scrubadub, scrubadub_spacy\n",
    "import snowflake.connector as sc\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "import vertexai\n",
    "import vertexai.preview.generative_models as generative_models\n",
    "from vertexai.generative_models import GenerativeModel, Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Component: List and download Files from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"boto3\", \"pandas\"]\n",
    ")\n",
    "def list_transcripts(aws_access_key: str, \n",
    "                     aws_secret_key: str, \n",
    "                     source_bucket: str, \n",
    "                     transcripts_location: str, \n",
    "                     max_objects: int) -> List[List[str]]:\n",
    "    \"\"\"Lists available transcripts from S3 bucket.\"\"\"\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=aws_access_key,\n",
    "        aws_secret_access_key=aws_secret_key\n",
    "    )\n",
    "\n",
    "    response = s3_client.list_objects_v2(Bucket=source_bucket, Prefix=transcripts_location)\n",
    "    \n",
    "    list_transcripts = []\n",
    "    for obj in response.get('Contents', []):\n",
    "        if obj['Key'].endswith('.json'):\n",
    "            list_transcripts.append([obj['Key'], str(obj['LastModified'])])\n",
    "            if len(list_transcripts) >= max_objects:\n",
    "                break\n",
    "\n",
    "    return list_transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Component: Process Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"boto3\", \"pandas\", \"numpy\", \"scipy\", \"transformers\", \"torch\"]\n",
    ")\n",
    "def create_intra_call_analysis(aws_access_key: str,\n",
    "                              aws_secret_key: str,\n",
    "                              source_bucket: str,\n",
    "                              file_key: str,\n",
    "                              contact_id: str) -> Dict:\n",
    "    \"\"\"Creates intra-call analysis data.\"\"\"\n",
    "    # Read transcript\n",
    "    s3_client = boto3.client('s3', \n",
    "                            aws_access_key_id=aws_access_key,\n",
    "                            aws_secret_access_key=aws_secret_key)\n",
    "    \n",
    "    response = s3_client.get_object(Bucket=source_bucket, Key=file_key)\n",
    "    transcript_data = json.loads(response['Body'].read().decode('utf-8'))\n",
    "    \n",
    "    # Process transcript (implementation from original notebook)\n",
    "    df_intra = process_transcript(transcript_data, contact_id)\n",
    "    df_sentiment = get_sentiment_scores(df_intra.caption.to_list())\n",
    "    df_intra = pd.concat([df_intra, df_sentiment], axis=1)\n",
    "    df_intra = get_different_times(df_intra)\n",
    "    \n",
    "    return df_intra.to_dict()\n",
    "\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform\", \"vertexai\", \"scrubadub\", \"scrubadub-spacy\"]\n",
    ")\n",
    "def create_inter_call_analysis(project_id: str,\n",
    "                              location: str,\n",
    "                              intra_call_data: Dict,\n",
    "                              transcript_data: Dict,\n",
    "                              last_modified_date: str) -> Dict:\n",
    "    \"\"\"Creates inter-call analysis data.\"\"\"\n",
    "    df_intra_call = pd.DataFrame.from_dict(intra_call_data)\n",
    "    \n",
    "    # Redact PII Data\n",
    "    scrubber = scrubadub.Scrubber()\n",
    "    scrubber.add_detector(scrubadub_spacy.detectors.SpacyEntityDetector)\n",
    "    df_intra_call.caption = df_intra_call.caption.apply(scrubber.clean)\n",
    "    \n",
    "    # Extract KPIs using Vertex AI\n",
    "    extractor = KPIExtractor(project_id, location)\n",
    "    transcript = \" \".join(df_intra_call.caption)\n",
    "    call_gen_kpis = extractor.extract_genai_kpis(transcript)\n",
    "    \n",
    "    # Create inter-call dictionary (implementation from original notebook)\n",
    "    inter_call_dict = create_inter_call_dict(df_intra_call, call_gen_kpis, transcript_data, last_modified_date)\n",
    "    \n",
    "    return inter_call_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Component: Write Data to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"snowflake-connector-python\", \"pandas\"]\n",
    ")\n",
    "def save_to_snowflake(conn_params: Dict,\n",
    "                      intra_call_data: Dict,\n",
    "                      inter_call_data: Dict):\n",
    "    \"\"\"Saves processed data to Snowflake.\"\"\"\n",
    "    conn = sc.connect(**conn_params)\n",
    "    \n",
    "    # Convert dictionaries back to dataframes\n",
    "    df_intra = pd.DataFrame.from_dict(intra_call_data)\n",
    "    df_inter = pd.DataFrame.from_dict(inter_call_data)\n",
    "    \n",
    "    # Save to Snowflake\n",
    "    write_pandas(conn, df_inter, 'SRC_GCP_INTER_CALLS')\n",
    "    write_pandas(conn, df_intra, 'SRC_GCP_INTRA_CALLS')\n",
    "    \n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Define Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='voice-ai-pipeline',\n",
    "    description='Pipeline for processing voice transcripts'\n",
    ")\n",
    "def voice_ai_pipeline(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    aws_access_key: str,\n",
    "    aws_secret_key: str,\n",
    "    source_bucket: str,\n",
    "    transcripts_location: str,\n",
    "    max_objects: int,\n",
    "    snowflake_conn_params: Dict\n",
    "):\n",
    "    # List available transcripts\n",
    "    list_task = list_transcripts(\n",
    "        aws_access_key=aws_access_key,\n",
    "        aws_secret_key=aws_secret_key,\n",
    "        source_bucket=source_bucket,\n",
    "        transcripts_location=transcripts_location,\n",
    "        max_objects=max_objects\n",
    "    )\n",
    "    \n",
    "    # Process each transcript\n",
    "    with dsl.ParallelFor(items=list_task.output) as transcript:\n",
    "        # Extract contact_id and last_modified_date\n",
    "        contact_id = dsl.RawArg(f\"{transcript[0].split('/')[-1].split('.')[0].split('analysis')[0].strip('_')}\")\n",
    "        last_modified = transcript[1]\n",
    "        \n",
    "        # Create intra-call analysis\n",
    "        intra_task = create_intra_call_analysis(\n",
    "            aws_access_key=aws_access_key,\n",
    "            aws_secret_key=aws_secret_key,\n",
    "            source_bucket=source_bucket,\n",
    "            file_key=transcript[0],\n",
    "            contact_id=contact_id\n",
    "        )\n",
    "        \n",
    "        # Create inter-call analysis\n",
    "        inter_task = create_inter_call_analysis(\n",
    "            project_id=project_id,\n",
    "            location=location,\n",
    "            intra_call_data=intra_task.output,\n",
    "            transcript_data=intra_task.outputs['transcript_data'],\n",
    "            last_modified_date=last_modified\n",
    "        )\n",
    "        \n",
    "        # Save results to Snowflake\n",
    "        save_to_snowflake(\n",
    "            conn_params=snowflake_conn_params,\n",
    "            intra_call_data=intra_task.output,\n",
    "            inter_call_data=inter_task.output\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Compile and Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=voice_ai_pipeline,\n",
    "    package_path='voice_ai_pipeline.json'\n",
    ")\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(project=project_id, location=location)\n",
    "\n",
    "# Create pipeline job\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name='voice-ai-pipeline-job',\n",
    "    template_path='voice_ai_pipeline.json',\n",
    "    parameter_values={\n",
    "        'project_id': project_id,\n",
    "        'location': location,\n",
    "        'aws_access_key': aws_access_key,\n",
    "        'aws_secret_key': aws_secret_key,\n",
    "        'source_bucket': source_bucket,\n",
    "        'transcripts_location': transcripts_location,\n",
    "        'max_objects': max_objects,\n",
    "        'snowflake_conn_params': conn_params\n",
    "    }\n",
    ")\n",
    "\n",
    "# Run the pipeline\n",
    "job.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
